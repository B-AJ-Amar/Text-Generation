{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4619727,"sourceType":"datasetVersion","datasetId":2688897},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re \nimport random\nimport string\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.ensemble import RandomForestClassifier\n\n\nimport tensorflow\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom tensorflow.keras.layers import Embedding\n\n\n!pip install nltk\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import ngrams\nfrom nltk.probability import FreqDist\n\n# Download necessary NLTK data files\nnltk.download('punkt')\nnltk.download('stopwords')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-02T18:32:28.458269Z","iopub.execute_input":"2024-08-02T18:32:28.458898Z","iopub.status.idle":"2024-08-02T18:32:46.057584Z","shell.execute_reply.started":"2024-08-02T18:32:28.458854Z","shell.execute_reply":"2024-08-02T18:32:46.055826Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n/kaggle/input/dailydialog-unlock-the-conversation-potential-in/validation.csv\n/kaggle/input/dailydialog-unlock-the-conversation-potential-in/train.csv\n/kaggle/input/dailydialog-unlock-the-conversation-potential-in/test.csv\n/kaggle/input/daigt-proper-train-dataset/train_drcat_03.csv\n/kaggle/input/daigt-proper-train-dataset/train_drcat_02.csv\n/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv\n/kaggle/input/daigt-proper-train-dataset/train_drcat_01.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 0. GloVe Embdeing\n---","metadata":{}},{"cell_type":"markdown","source":"### download ...","metadata":{}},{"cell_type":"code","source":"# download stanford GloVe\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2024-08-02T17:52:27.588732Z","iopub.execute_input":"2024-08-02T17:52:27.589776Z","iopub.status.idle":"2024-08-02T17:55:33.129408Z","shell.execute_reply.started":"2024-08-02T17:52:27.589738Z","shell.execute_reply":"2024-08-02T17:55:33.127879Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2024-08-02 17:52:28--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2024-08-02 17:52:28--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2024-08-02 17:52:28--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: 'glove.6B.zip'\n\nglove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 38s  \n\n2024-08-02 17:55:07 (5.19 MB/s) - 'glove.6B.zip' saved [862182613/862182613]\n\nArchive:  glove.6B.zip\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt       \n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_dim = 300\ndef load_glove_model(glove_file):\n    print(\"Loading GloVe Model...\")\n    model = {}\n    with open(glove_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            split_line = line.split()\n            word = split_line[0]\n            embedding = np.array([float(val) for val in split_line[1:]])\n            model[word] = embedding\n            \n    model[\"<unk>\"] = np.array([float(0) for _ in range(embedding_dim)] )\n    print(\"Done.\", len(model), \" words loaded!\")\n    return model\n\nglove_model = load_glove_model(f\"glove.6B.{str(embedding_dim)}d.txt\")  \n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:13:19.168603Z","iopub.execute_input":"2024-08-02T18:13:19.169070Z","iopub.status.idle":"2024-08-02T18:14:08.451491Z","shell.execute_reply.started":"2024-08-02T18:13:19.169034Z","shell.execute_reply":"2024-08-02T18:14:08.450037Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Loading GloVe Model...\nDone. 400001  words loaded!\n","output_type":"stream"}]},{"cell_type":"code","source":"word_index = list(glove_model.keys())\nvocab_size = len(word_index)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:14:16.925755Z","iopub.execute_input":"2024-08-02T18:14:16.926444Z","iopub.status.idle":"2024-08-02T18:14:16.949034Z","shell.execute_reply.started":"2024-08-02T18:14:16.926388Z","shell.execute_reply":"2024-08-02T18:14:16.947108Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### embeding matrix","metadata":{}},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor i in range(len(word_index)):\n    embedding_matrix[i] =  glove_model.get(word_index[i])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:14:22.169222Z","iopub.execute_input":"2024-08-02T18:14:22.169776Z","iopub.status.idle":"2024-08-02T18:14:24.175439Z","shell.execute_reply.started":"2024-08-02T18:14:22.169722Z","shell.execute_reply":"2024-08-02T18:14:24.173783Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data\n---","metadata":{}},{"cell_type":"code","source":"\ndata1 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_01.csv\", usecols =[\"text\"])\ndata2 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_02.csv\", usecols =[\"text\"])\ndata3 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_03.csv\", usecols =[\"text\"])\ndata4 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv\", usecols =[\"text\"])\n\n\ntrain_data = pd.concat([data1,data2,data3,data4])\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:32:54.164586Z","iopub.execute_input":"2024-08-02T18:32:54.165113Z","iopub.status.idle":"2024-08-02T18:33:04.687543Z","shell.execute_reply.started":"2024-08-02T18:32:54.165064Z","shell.execute_reply":"2024-08-02T18:33:04.686271Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                text\n0  There are alot reasons to keep our the despise...\n1  Driving smart cars that drive by themself has ...\n2  Dear Principal,\\n\\nI believe that students at ...\n3  Dear Principal,\\n\\nCommunity service should no...\n4  My argument for the development of the driverl...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>There are alot reasons to keep our the despise...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Driving smart cars that drive by themself has ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dear Principal,\\n\\nI believe that students at ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dear Principal,\\n\\nCommunity service should no...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>My argument for the development of the driverl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data = train_data.dropna()\ndata_size = len(train_data.text)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:37:06.106565Z","iopub.execute_input":"2024-08-02T18:37:06.107430Z","iopub.status.idle":"2024-08-02T18:37:06.221297Z","shell.execute_reply.started":"2024-08-02T18:37:06.107369Z","shell.execute_reply":"2024-08-02T18:37:06.219941Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def data_proccesing(data,ngram=4):\n    X,Y = [],[]\n    for row in data:\n        text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", row)\n        tokens = word_tokenize(text.lower())\n\n        sentence = []\n        for token in tokens:\n            try:\n                sentence.append(word_index.index(token))\n            except:\n                sentence.append(word_index.index(\"<unk>\"))             \n        \n        ng = ngrams(sentence, ngram)\n        for i in ng:\n            X.append(i[:-1])\n            Y.append(i[-1])\n    \n    X = np.array(X)\n    Y = np.array(Y)\n    \n    return X,Y\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:52:37.614136Z","iopub.execute_input":"2024-08-02T18:52:37.614647Z","iopub.status.idle":"2024-08-02T18:52:37.625628Z","shell.execute_reply.started":"2024-08-02T18:52:37.614612Z","shell.execute_reply":"2024-08-02T18:52:37.623825Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## 3. Model","metadata":{}},{"cell_type":"code","source":"model = Sequential ([\n    Embedding(input_dim=vocab_size,\n              output_dim=embedding_dim,\n              weights=[embedding_matrix],\n              trainable=False),\n    LSTM(256,return_sequences=True),\n    Dropout(0.2),\n    LSTM(128,return_sequences=False),\n    Dropout(0.2),\n    Dense(200, activation='relu'),\n    Dropout(0.2),\n    Dense(vocab_size, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:36:05.651291Z","iopub.execute_input":"2024-08-02T18:36:05.651795Z","iopub.status.idle":"2024-08-02T18:36:08.230020Z","shell.execute_reply.started":"2024-08-02T18:36:05.651758Z","shell.execute_reply":"2024-08-02T18:36:08.228576Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### training\n","metadata":{}},{"cell_type":"code","source":"history = []\n\ndef train(epochs =10,sample_size=0.1):\n    global history,data_size\n    if sample_size>data_size-1 :\n        print(\"too large sample size\")\n        return \n    if sample_size>1:\n        r = random.randint(0,data_size-sample_size)\n    elif 0<sample_size <= 1:\n        sample_size = int(sample_size * data_size)\n        r = random.randint(0,data_size-sample_size)\n    else: \n        print(\"invalid parameters\")\n        return \n        \n    sample_x,sample_y = data_proccesing(train_data.text[r:r+sample_size])\n    his = model.fit(sample_x,sample_y,epochs = epochs ,validation_split=0.2)\n    \n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\n    ax[0].plot(his.history['accuracy'])\n    ax[0].plot(his.history['val_accuracy'])\n    ax[0].legend(['accuracy', 'val'], loc='upper left')\n\n    ax[1].plot(his.history['loss'])\n    ax[1].plot(his.history['val_loss'])\n    ax[1].legend(['loss', 'val_loss'], loc='upper left')\n\n    plt.show()\n    \n#     history.append(his.history)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:48:26.400045Z","iopub.execute_input":"2024-08-02T18:48:26.400543Z","iopub.status.idle":"2024-08-02T18:48:26.414666Z","shell.execute_reply.started":"2024-08-02T18:48:26.400507Z","shell.execute_reply":"2024-08-02T18:48:26.413031Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2024-08-02T18:52:50.141617Z","iopub.execute_input":"2024-08-02T18:52:50.143665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### prediction","metadata":{}},{"cell_type":"code","source":"\ndef pred(words,lenght=100):\n    word_arr = list(words.split())\n    indexs = [word_index.index(w.lower()) for w in word_arr ]\n    vec = np.array(indexs).reshape(1,3,1)\n    prd = np.argmax(model.predict(vec,verbose=0))\n    prd = word_index[prd] \n    \n    \n    return prd","metadata":{"execution":{"iopub.status.busy":"2024-08-02T00:51:57.681559Z","iopub.execute_input":"2024-08-02T00:51:57.682026Z","iopub.status.idle":"2024-08-02T00:51:57.689297Z","shell.execute_reply.started":"2024-08-02T00:51:57.681989Z","shell.execute_reply":"2024-08-02T00:51:57.688169Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"\ncnt = 0\narr = [\"my\",\"name\",'is']\nprint(arr[0],arr[1],end=\" \")\nwhile cnt<100:\n    prd = pred(\" \".join(arr))\n    arr[0] = arr[1]\n    arr[1] = prd\n    cnt +=1\n    print(prd,end=\" \")","metadata":{"execution":{"iopub.status.busy":"2024-08-02T00:52:57.707583Z","iopub.execute_input":"2024-08-02T00:52:57.708633Z","iopub.status.idle":"2024-08-02T00:53:07.925693Z","shell.execute_reply.started":"2024-08-02T00:52:57.708586Z","shell.execute_reply":"2024-08-02T00:53:07.924301Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"my name it good you i that it but dinner with know sounds they me , they suggest , over cake suggest silly hear last go i <unk> it us us . . good to thats with idea sounds hear fun go . <unk> to with with sounds sounds me me they they , , after suggest i our it thats you idea that hear with go sounds <unk> me are they shall , ? suggest that over with cake dinner know know tempting tempting and and too of s . . good to thats with idea sounds hear fun go . ","output_type":"stream"}]},{"cell_type":"markdown","source":"### Note : \nThis model requires a large amount of data to function as expected. It is intended for learning purposes only.\n","metadata":{}}]}